{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f3fe049a",
      "metadata": {
        "id": "f3fe049a"
      },
      "source": [
        "# DreamerV1 Code Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971eb721",
      "metadata": {
        "id": "971eb721"
      },
      "source": [
        "## Core Algorithm in Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "523506a6",
      "metadata": {
        "id": "523506a6"
      },
      "source": [
        "### Dynamics Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f754824",
      "metadata": {
        "id": "0f754824"
      },
      "source": [
        "### Behavior Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2591812",
      "metadata": {
        "id": "f2591812"
      },
      "source": [
        "### Environment Interaction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54b608e",
      "metadata": {
        "id": "f54b608e"
      },
      "source": [
        "## Models\n",
        "\n",
        "The latent world model that Dreamer uses is composed of three components:\n",
        "- Representation Model\n",
        "- Transition Model\n",
        "- Reward Model\n",
        "\n",
        "An additional observation model is used as a training signal for image reconstruction loss.\n",
        "\n",
        "The behavior is learned via an action/value model like in actor/critic RL algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f44bc1",
      "metadata": {
        "id": "67f44bc1"
      },
      "source": [
        "### Representation Model\n",
        "\n",
        "The representation model encodes actions and observations in order to output a continuous, vector-valued state with Markovian transitions.\n",
        "\n",
        "Mathematically it is the probability distribution over the real data of a state $s_t$ given previous state/action ($s_{t-1}$, $a_{t-1}$) and current observation $o_t$:\n",
        "\n",
        "> $p_{\\theta}(s_t|s_{t-1},a_{t-1},o_t)$ where $p$ indicates a probability distribution over real experience data\n",
        "\n",
        "\n",
        "In DreamerV1 it is represented as a CNN encoder and an RSSM (Recurrent State Space Model)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder is responsible for compressing the raw pixel space into a lower dimensional feature vector\n",
        "#   This provides the observation part in the representation model\n",
        "self.encoder = models.ConvEncoder(input_shape=self.obs_size).to(self.device)\n",
        "\n",
        "# The encoder's output is stored in eb_obs variable which is used by the RSSM later\n",
        "eb_obs = self.encoder(b_obs)"
      ],
      "metadata": {
        "id": "YcVWuDgNEpyx"
      },
      "id": "YcVWuDgNEpyx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the encoder processes an image from the environment, it's output feature vector is used in the RSSM model. The RSSM model effectively acts as a non-linear kalman filter. First, a deterministic prior belief of the world dynamics is used to propagate the past state/action pair into the current time step. Then a correction to this prediction is applied by using the observation taken from the real environment.\n",
        "\n",
        "It is important to remember that this process operates with real transition data, not the imagined data."
      ],
      "metadata": {
        "id": "uFhcWNI7wode"
      },
      "id": "uFhcWNI7wode"
    },
    {
      "cell_type": "code",
      "source": [
        "# RSSM combines the feature vector from the encoder with the previous state/action to create the new state s_t\n",
        "self.rssm = models.RSSM(self.config.main.stochastic_size,\n",
        "                                self.config.main.embedded_obs_size,\n",
        "                                self.config.main.deterministic_size,\n",
        "                                self.config.main.hidden_units,\n",
        "                                self.action_size).to(self.device)\n",
        "\n",
        "# Prior belief is propagated to current time (prediction)\n",
        "deterministic = self.rssm.recurrent(posterior, b_a[:, t-1, :], deterministic)\n",
        "\n",
        "# The posterior is calculated given the new observation, this is now a stochastic state (correction)\n",
        "posterior_dist, posterior = self.rssm.representation(eb_obs[:, t, :], deterministic)"
      ],
      "metadata": {
        "id": "mJSqQV0kvYBU"
      },
      "id": "mJSqQV0kvYBU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more details check the function called **dynamic_learning** in the Dreamer class"
      ],
      "metadata": {
        "id": "rnHu__w-yxbz"
      },
      "id": "rnHu__w-yxbz"
    },
    {
      "cell_type": "markdown",
      "id": "5836a47a",
      "metadata": {
        "id": "5836a47a"
      },
      "source": [
        "### Reward Model\n",
        "\n",
        "The reward model operates in the imagined latent domain and is used to predict rewards along imagined trajectories.\n",
        "\n",
        "Mathematically it is the probability of the current reward given current state:\n",
        "> $q_{\\theta}(r_t|s_t)$ where $q$ is a probability distribution over imagined data\n",
        "\n",
        "\n",
        "In DreamerV1 it is represented as a simple fully connected network."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The reward model is defined as an MLP with 2 layers (also a dense network)\n",
        "self.reward = models.RewardNet(self.config.main.stochastic_size + self.config.main.deterministic_size,\n",
        "                                       self.config.main.hidden_units).to(self.device)\n",
        "# It's implementation in RewardNet's constructor:\n",
        "self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            activation(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )"
      ],
      "metadata": {
        "id": "HeYDSToKzEU5"
      },
      "id": "HeYDSToKzEU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the reward model derives it's ability to estimate imagined states by learning from real experiences first. We see in the function **dynamic_learning** that a log probability loss is computed between predicted rewards of real states and the true reward taken from the replay buffer."
      ],
      "metadata": {
        "id": "CESSd6QN1cG6"
      },
      "id": "CESSd6QN1cG6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable b_r is the true reward from the replay buffer of real experiences\n",
        "rewards = self.reward(posteriors, deterministics)\n",
        "rewards_dist = torch.distributions.Normal(rewards, 1)\n",
        "rewards_dist = torch.distributions.Independent(rewards_dist, 1)\n",
        "rewards_loss = rewards_dist.log_prob(b_r[:, 1:]).mean()"
      ],
      "metadata": {
        "id": "_e1xVGCg1blk"
      },
      "id": "_e1xVGCg1blk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the imagined states, we see the reward model is used to predict the reward of the imagined trajectories. This is in the function called **behavioral_learning**"
      ],
      "metadata": {
        "id": "PXLaRB1z3S0P"
      },
      "id": "PXLaRB1z3S0P"
    },
    {
      "cell_type": "code",
      "source": [
        "# In the behavioral_learning function the imagined trajectories are calculated and given to the reward model to predict reward\n",
        "rewards = self.reward(state_trajectories, deterministics_trajectories)\n",
        "rewards_dist = torch.distributions.Normal(rewards, 1)\n",
        "rewards_dist = torch.distributions.Independent(rewards_dist, 1)\n",
        "rewards = rewards_dist.mode"
      ],
      "metadata": {
        "id": "RGMIMahE3SiU"
      },
      "id": "RGMIMahE3SiU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "871863e7",
      "metadata": {
        "id": "871863e7"
      },
      "source": [
        "### Transition Model\n",
        "\n",
        "The transition model is used to predict a new imagined latent states given a previous state and action. It can be used to generate full imagined trajectories of future states without real-world data, hence the dreamer aspect of DreamerV1.\n",
        "\n",
        "Mathematically it is the probability of current state given previous state/action:\n",
        "> $q_{\\theta}(s_t|s_{t-1},a_{t-1})$\n",
        "\n",
        "It also uses the RSSM model that the representation model is defined from."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to how the reward model is trained from the real experience data, the transition also is used in the **dynamic_learning** in order to learn physics/logic of the environment. The transition model is then used during the **behavior_learning** function to provide imagined trajectories."
      ],
      "metadata": {
        "id": "6tkPouEW6xsA"
      },
      "id": "6tkPouEW6xsA"
    },
    {
      "cell_type": "code",
      "source": [
        "# In dynamic_learning function, the next stochastic state is calculated by applying transition model's current learned dynamics\n",
        "prior_dist, prior = self.rssm.transition(deterministic)\n",
        "\n",
        "# Transition model used to dream new states in behavior_learning function\n",
        "for t in range(self.config.main.horizon):\n",
        "    action = self.actor(state, deterministics)\n",
        "    deterministics = self.rssm.recurrent(state, action, deterministics)\n",
        "    _, state = self.rssm.transition(deterministics)\n",
        "    state_trajectories[:, t, :] = state\n",
        "    deterministics_trajectories[:, t, :] = deterministics"
      ],
      "metadata": {
        "id": "at1Bwk8-7UBP"
      },
      "id": "at1Bwk8-7UBP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6e333563",
      "metadata": {
        "id": "6e333563"
      },
      "source": [
        "### Observation Model\n",
        "\n",
        "decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb727182",
      "metadata": {
        "id": "eb727182"
      },
      "source": [
        "### Action Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f71d909",
      "metadata": {
        "id": "3f71d909"
      },
      "source": [
        "### Value Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bcfe123",
      "metadata": {
        "id": "6bcfe123"
      },
      "source": [
        "## Latent Imagination"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd14608b",
      "metadata": {
        "id": "dd14608b"
      },
      "source": [
        "## Running xperiments"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}