{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f3fe049a",
      "metadata": {
        "id": "f3fe049a"
      },
      "source": [
        "# DreamerV1 Code Explanation\n",
        "The Dreamer paper (“Dream to Control: Learning Behaviors by Latent Imagination”) boils down to three parts:\n",
        "\n",
        "1) Dynamics learning — learn a world model from real data.\n",
        "\n",
        "2) Behavior learning — use that model to imagine rollouts and train the actor/critic.\n",
        "\n",
        "3) Environment interaction — collect fresh data with the current policy and repeat.\n",
        "\n",
        "In the following sections, we will walk through these phases and explain the components and logic flow for each of these phases, beginning with **Dynamics Learning**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "523506a6",
      "metadata": {
        "id": "523506a6"
      },
      "source": [
        "##1. Dynamics Learning\n",
        "\n",
        "Initially, Dreamer learns a **world model** that captures how the environment behaves, using real data.  \n",
        "Instead of learning directly from raw pixels, dreamer first encodes observations, like images, into compact latent states and learns how they change over time.\n",
        "\n",
        "The world model includes:\n",
        "- an **Encoder** and **RSSM**, which together is implemented by the Representation and Transition Models,\n",
        "- a **Reward Model** that predicts rewards from latent states, and\n",
        "- a **Decoder** that is implemented by an Observation Model, that reconstructs images for a training signal.\n",
        "\n",
        "All of these different parts and the models used to implement them are explained in detail down below.\n",
        "\n",
        "Once this model is trained, Dreamer can generate future states entirely within latent space which then allows it to imagine what the outcomes of actions would be without additional real-world information.\n",
        "\n",
        "Below is the pseudocode overview of Dreamer's world model learning phase. This function shows how the main components (Representation,Transition, Reward, and Observation models) work together.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamics_learning(self, replay_buffer):\n",
        "\n",
        "    # Sample a batch of real trajectories from replay buffer\n",
        "    b_obs, b_act, b_rew = replay_buffer.sample(self.config.main.batch_size)\n",
        "\n",
        "    # Perform the learning step (computes different losses and updates the world model)\n",
        "    losses = self.dynamic_learning_step(b_obs, b_act, b_rew)\n",
        "\n",
        "    # Return the computed loss values for logging or display\n",
        "    return losses\n",
        "\n",
        "def dynamic_learning_step(self, b_obs, b_act, b_rew):\n",
        "\n",
        "    emb_obs = self.encoder(b_obs)  # Encoded latent observations\n",
        "\n",
        "    # Initialize latent states\n",
        "    deter_state = self.rssm.init_deterministic(b_obs.size(0)).to(self.device)\n",
        "    post_state  = self.rssm.init_stochastic(b_obs.size(0)).to(self.device)\n",
        "\n",
        "    # Initialize loss terms\n",
        "    reconstruction_loss, reward_prediction_loss, consistency_alignment_loss = 0, 0, 0\n",
        "\n",
        "    for t in range(b_obs.size(1)):\n",
        "        # Predict next state and correct it with current observation\n",
        "        deter_state = self.rssm.recurrent(\n",
        "            post_state,\n",
        "            b_act[:, t-1, :] if t > 0 else 0,\n",
        "            deter_state\n",
        "        )\n",
        "        prior_dist, _ = self.rssm.transition(deter_state)\n",
        "        post_dist, post_state = self.rssm.representation(emb_obs[:, t, :], deter_state)\n",
        "\n",
        "        # Compute individual losses\n",
        "        reconstruction_loss += -self.decoder(post_state, deter_state).log_prob(b_obs[:, t]).mean()\n",
        "        reward_prediction_loss += -(self.reward(post_state, deter_state)).mean()\n",
        "        consistency_alignment_loss += torch.distributions.kl.kl_divergence(post_dist, prior_dist).mean()\n",
        "\n",
        "    total_loss = reconstruction_loss + reward_prediction_loss + consistency_alignment_loss\n",
        "\n",
        "    return {\n",
        "        \"total_loss\": total_loss,\n",
        "        \"reconstruction_loss\": reconstruction_loss,\n",
        "        \"reward_prediction_loss\": reward_prediction_loss,\n",
        "        \"consistency_alignment_loss\": consistency_alignment_loss\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ZKD_oDoBnmHn"
      },
      "id": "ZKD_oDoBnmHn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above shows how Dreamer trains its world model step by step.  \n",
        "Each part of the model learns a different skill from real experience:  \n",
        "\n",
        "- The **Representation Model** turns raw images into smaller “latent” features that are easier to work with.  \n",
        "- The **Transition Model** learns how those features change when the agent takes an action — basically predicting what comes next.  \n",
        "- The **Reward Model** learns to guess the reward the agent would get in that situation.  \n",
        "- The **Observation Model** tries to rebuild the original image, helping the model remember what details matter.  \n",
        "\n",
        "Each of these parts produces its own loss:  \n",
        "- **Reconstruction loss** (how well the model rebuilds the image),  \n",
        "- **Reward loss** (how well it predicts the reward), and  \n",
        "- **Consistency loss** (how consistent its imagined state is with the real one).  \n",
        "\n",
        "By combining these, Dreamer gradually learns how the environment works inside a compact, internal world.\n",
        "\n",
        "Next, we’ll explore each of these models to see what they do.\n"
      ],
      "metadata": {
        "id": "uGwPXPUm-pWO"
      },
      "id": "uGwPXPUm-pWO"
    },
    {
      "cell_type": "markdown",
      "id": "f54b608e",
      "metadata": {
        "id": "f54b608e"
      },
      "source": [
        "## Models\n",
        "\n",
        "The latent world model that Dreamer uses is composed of three components:\n",
        "- Representation Model\n",
        "- Transition Model\n",
        "- Reward Model\n",
        "\n",
        "An additional observation model is used as a training signal for image reconstruction loss.\n",
        "\n",
        "The behavior is learned via an action/value model like in actor/critic RL algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f44bc1",
      "metadata": {
        "id": "67f44bc1"
      },
      "source": [
        "### Representation Model\n",
        "\n",
        "The representation model encodes actions and observations in order to output a continuous, vector-valued state with Markovian transitions.\n",
        "\n",
        "Mathematically it is the probability distribution over the real data of a state $s_t$ given previous state/action ($s_{t-1}$, $a_{t-1}$) and current observation $o_t$:\n",
        "\n",
        "> $p_{\\theta}(s_t|s_{t-1},a_{t-1},o_t)$ where $p$ indicates a probability distribution over real experience data\n",
        "\n",
        "\n",
        "In DreamerV1 it is represented as a CNN encoder and an RSSM (Recurrent State Space Model)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder is responsible for compressing the raw pixel space into a lower dimensional feature vector\n",
        "#   This provides the observation part in the representation model\n",
        "self.encoder = models.ConvEncoder(input_shape=self.obs_size).to(self.device)\n",
        "\n",
        "# The encoder's output is stored in eb_obs variable which is used by the RSSM later\n",
        "eb_obs = self.encoder(b_obs)"
      ],
      "metadata": {
        "id": "YcVWuDgNEpyx"
      },
      "id": "YcVWuDgNEpyx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the encoder processes an image from the environment, it's output feature vector is used in the RSSM model. The RSSM model effectively acts as a non-linear kalman filter. First, a deterministic prior belief of the world dynamics is used to propagate the past state/action pair into the current time step. Then a correction to this prediction is applied by using the observation taken from the real environment.\n",
        "\n",
        "It is important to remember that this process operates with real transition data, not the imagined data."
      ],
      "metadata": {
        "id": "uFhcWNI7wode"
      },
      "id": "uFhcWNI7wode"
    },
    {
      "cell_type": "code",
      "source": [
        "# RSSM combines the feature vector from the encoder with the previous state/action to create the new state s_t\n",
        "self.rssm = models.RSSM(self.config.main.stochastic_size,\n",
        "                                self.config.main.embedded_obs_size,\n",
        "                                self.config.main.deterministic_size,\n",
        "                                self.config.main.hidden_units,\n",
        "                                self.action_size).to(self.device)\n",
        "\n",
        "# Prior belief is propagated to current time (prediction)\n",
        "deterministic = self.rssm.recurrent(posterior, b_a[:, t-1, :], deterministic)\n",
        "\n",
        "# The posterior is calculated given the new observation, this is now a stochastic state (correction)\n",
        "posterior_dist, posterior = self.rssm.representation(eb_obs[:, t, :], deterministic)"
      ],
      "metadata": {
        "id": "mJSqQV0kvYBU"
      },
      "id": "mJSqQV0kvYBU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more details check the function called **dynamic_learning** in the Dreamer class"
      ],
      "metadata": {
        "id": "rnHu__w-yxbz"
      },
      "id": "rnHu__w-yxbz"
    },
    {
      "cell_type": "markdown",
      "id": "5836a47a",
      "metadata": {
        "id": "5836a47a"
      },
      "source": [
        "### Reward Model\n",
        "\n",
        "The reward model operates in the imagined latent domain and is used to predict rewards along imagined trajectories.\n",
        "\n",
        "Mathematically it is the probability of the current reward given current state:\n",
        "> $q_{\\theta}(r_t|s_t)$ where $q$ is a probability distribution over imagined data\n",
        "\n",
        "\n",
        "In DreamerV1 it is represented as a simple fully connected network."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The reward model is defined as an MLP with 2 layers (also a dense network)\n",
        "self.reward = models.RewardNet(self.config.main.stochastic_size + self.config.main.deterministic_size,\n",
        "                                       self.config.main.hidden_units).to(self.device)\n",
        "# It's implementation in RewardNet's constructor:\n",
        "self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            activation(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )"
      ],
      "metadata": {
        "id": "HeYDSToKzEU5"
      },
      "id": "HeYDSToKzEU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the reward model derives it's ability to estimate imagined states by learning from real experiences first. We see in the function **dynamic_learning** that a log probability loss is computed between predicted rewards of real states and the true reward taken from the replay buffer."
      ],
      "metadata": {
        "id": "CESSd6QN1cG6"
      },
      "id": "CESSd6QN1cG6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable b_r is the true reward from the replay buffer of real experiences\n",
        "rewards = self.reward(posteriors, deterministics)\n",
        "rewards_dist = torch.distributions.Normal(rewards, 1)\n",
        "rewards_dist = torch.distributions.Independent(rewards_dist, 1)\n",
        "rewards_loss = rewards_dist.log_prob(b_r[:, 1:]).mean()"
      ],
      "metadata": {
        "id": "_e1xVGCg1blk"
      },
      "id": "_e1xVGCg1blk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the imagined states, we see the reward model is used to predict the reward of the imagined trajectories. This is in the function called **behavioral_learning**"
      ],
      "metadata": {
        "id": "PXLaRB1z3S0P"
      },
      "id": "PXLaRB1z3S0P"
    },
    {
      "cell_type": "code",
      "source": [
        "# In the behavioral_learning function the imagined trajectories are calculated and given to the reward model to predict reward\n",
        "rewards = self.reward(state_trajectories, deterministics_trajectories)\n",
        "rewards_dist = torch.distributions.Normal(rewards, 1)\n",
        "rewards_dist = torch.distributions.Independent(rewards_dist, 1)\n",
        "rewards = rewards_dist.mode"
      ],
      "metadata": {
        "id": "RGMIMahE3SiU"
      },
      "id": "RGMIMahE3SiU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "871863e7",
      "metadata": {
        "id": "871863e7"
      },
      "source": [
        "### Transition Model\n",
        "\n",
        "The transition model is used to predict a new imagined latent states given a previous state and action. It can be used to generate full imagined trajectories of future states without real-world data, hence the dreamer aspect of DreamerV1.\n",
        "\n",
        "Mathematically it is the probability of current state given previous state/action:\n",
        "> $q_{\\theta}(s_t|s_{t-1},a_{t-1})$\n",
        "\n",
        "It also uses the RSSM model that the representation model is defined from."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to how the reward model is trained from the real experience data, the transition also is used in the **dynamic_learning** in order to learn physics/logic of the environment. The transition model is then used during the **behavior_learning** function to provide imagined trajectories."
      ],
      "metadata": {
        "id": "6tkPouEW6xsA"
      },
      "id": "6tkPouEW6xsA"
    },
    {
      "cell_type": "code",
      "source": [
        "# In dynamic_learning function, the next stochastic state is calculated by applying transition model's current learned dynamics\n",
        "prior_dist, prior = self.rssm.transition(deterministic)\n",
        "\n",
        "# Transition model used to dream new states in behavior_learning function\n",
        "for t in range(self.config.main.horizon):\n",
        "    action = self.actor(state, deterministics)\n",
        "    deterministics = self.rssm.recurrent(state, action, deterministics)\n",
        "    _, state = self.rssm.transition(deterministics)\n",
        "    state_trajectories[:, t, :] = state\n",
        "    deterministics_trajectories[:, t, :] = deterministics"
      ],
      "metadata": {
        "id": "at1Bwk8-7UBP"
      },
      "id": "at1Bwk8-7UBP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6e333563",
      "metadata": {
        "id": "6e333563"
      },
      "source": [
        "### Observation Model\n",
        "\n",
        "The observation model is used as a training signal in the world model learning phase. Specifically, it learns from the real image data and tries to learn how to reconstruct them.\n",
        "\n",
        "In the code it is a decoder that takes latent states and converts them into a probability distribution of possible images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the decoder\n",
        "self.decoder = models.ConvDecoder(self.config.main.stochastic_size,\n",
        "                                  self.config.main.deterministic_size,\n",
        "                                  out_shape=self.obs_size).to(self.device)\n",
        "\n",
        "# Calculating the reconstruction loss\n",
        "reconstruct_dist = self.decoder(posteriors, deterministics, mps_flatten)"
      ],
      "metadata": {
        "id": "_lJw6TR5ySp3"
      },
      "id": "_lJw6TR5ySp3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Behavior Learning\n",
        "\n",
        "After the world model is trained, Dreamer uses it to learn how to act — this is the **Behavior Learning** phase.  \n",
        "Instead of exploring in the real environment, the agent uses its learned **world model** to **imagine trajectories** inside the latent space.\n",
        "\n",
        "Here’s what happens in this phase:\n",
        "\n",
        "1. The **Actor (Action Model)** proposes actions based on the current latent state.  \n",
        "2. The **Transition Model** imagines what the next latent state would be if that action were taken.  \n",
        "3. The **Reward Model** predicts the reward the agent would get in that imagined step.  \n",
        "4. The **Critic (Value Model)** estimates how good each imagined state is in the long term.  \n",
        "5. The Actor and Critic are trained together — the Actor to maximize imagined returns, and the Critic to match its value predictions to those returns.\n",
        "\n",
        "This process is enhanced by **latent imagination** — the agent “dreams” future experiences inside its world model to keep learning, even without interacting with the real environment.\n"
      ],
      "metadata": {
        "id": "ypcJDwcF_2tN"
      },
      "id": "ypcJDwcF_2tN"
    },
    {
      "cell_type": "code",
      "source": [
        "def behavior_learning(self, start_state, start_deter):\n",
        "\n",
        "    # Generate imagined trajectories\n",
        "    actions, rewards, values = [], [], []\n",
        "    state, deter = start_state, start_deter\n",
        "\n",
        "    for t in range(self.config.main.horizon):\n",
        "        action = self.actor(state, deter)\n",
        "        deter = self.rssm.recurrent(state, action, deter)\n",
        "        _, state = self.rssm.transition(deter)\n",
        "\n",
        "        reward = self.reward(state, deter)\n",
        "        value = self.critic(state, deter)\n",
        "        actions.append(action); rewards.append(reward); values.append(value)\n",
        "\n",
        "    # Compute returns and optimize actor and critic\n",
        "    returns = td_lambda(rewards, torch.ones_like(rewards)*self.config.main.discount, values, self.config.main.lambda_, self.device)\n",
        "    self.update_actor_critic(returns, values)\n"
      ],
      "metadata": {
        "id": "Kgk2xEi9Ar59"
      },
      "id": "Kgk2xEi9Ar59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function above puts the Behavior Learning process into action. It shows how Dreamer loops through imagined steps: using the actor to choose actions, the world model to predict what happens next, and the critic to estimate long-term value. The returns from these imagined rollouts are then used to update both the actor and critic networks.  \n",
        "\n",
        "Next, we’ll look at how the **Action Model**, **Value Model**, and the **Latent Imagination** process make this learning possible.\n"
      ],
      "metadata": {
        "id": "_mcEC9qXBOWo"
      },
      "id": "_mcEC9qXBOWo"
    },
    {
      "cell_type": "markdown",
      "id": "eb727182",
      "metadata": {
        "id": "eb727182"
      },
      "source": [
        "### Action Model\n",
        "\n",
        "The action model is an actor model in the actor-critic algorithm. It is defined as a simple neural network with two linear layers. It is used here to predict an action given the latent state. As shown in the paper, we look to do this by sampling from the action distribution but also want to use be able to backprop through this operation.\n",
        "\n",
        "$a_\\tau \\sim q_\\phi(a_\\tau \\mid s_\\tau)$\n",
        "\n",
        "$a_\\tau = \\tanh\\!\\big(\\mu_\\phi(x_\\tau) + \\sigma_\\phi(s_\\tau) \\, \\epsilon\\big) where\n",
        "\\quad \\epsilon \\sim N(0, I)$\n",
        "\n",
        "The actor does not interact with the actual environment, but only with the latent state during imagination."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "def __init__(self,\n",
        "              latent_size,\n",
        "              hidden_size,\n",
        "              action_size,\n",
        "              discrete=True,\n",
        "              activation=nn.ELU,\n",
        "              min_std=1e-4,\n",
        "              init_std=5,\n",
        "              mean_scale=5):\n",
        "\n",
        "    super().__init__()\n",
        "    self.latent_size = latent_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.action_size = (action_size if discrete else action_size*2)\n",
        "    self.discrete = discrete\n",
        "    self.min_std=min_std\n",
        "    self.init_std = init_std\n",
        "    self.mean_scale = mean_scale\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(latent_size, hidden_size),\n",
        "        activation(),\n",
        "        nn.Linear(hidden_size, self.action_size)\n",
        "    )"
      ],
      "metadata": {
        "id": "1GqU4gwBCiQk"
      },
      "id": "1GqU4gwBCiQk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how the actor is trained during imagination. We optimize the model via backpropogation of the gradients in order to maximize returns of the predicted actions during the trajectories.\n",
        "\n",
        "The objective from the paper for actor's parameters $\\phi$ is:\n",
        "\n",
        "$\\max_{\\phi} \\;\n",
        "\\mathbb{E}_{q_\\theta, q_\\phi}\\!\\left[\n",
        "  \\sum_{\\tau = t}^{t + H} V_\\lambda(s_\\tau)\n",
        "\\right]$"
      ],
      "metadata": {
        "id": "kn9Ei2AgfP_9"
      },
      "id": "kn9Ei2AgfP_9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiation of the action model\n",
        "self.actor = models.Actor(self.config.main.stochastic_size + self.config.main.deterministic_size,\n",
        "                                  self.config.main.hidden_units,\n",
        "                                  self.action_size,\n",
        "                                  self.config.env.discrete).to(self.device)\n",
        "\n",
        "# Get action during imagination from the actor model\n",
        "action = self.actor(state, deterministics)\n",
        "\n",
        "# Compute loss based on discoutned returns from the simulated trajectories\n",
        "actor_loss = -(discount * returns).mean()\n",
        "self.actor_optimizer.zero_grad()\n",
        "actor_loss.backward()\n",
        "nn.utils.clip_grad_norm_(\n",
        "    self.actor.parameters(),\n",
        "    self.config.main.clip_grad,\n",
        "    norm_type=self.config.main.grad_norm_type,\n",
        ")\n",
        "self.actor_optimizer.step()"
      ],
      "metadata": {
        "id": "J7JjzNEnDP5y"
      },
      "id": "J7JjzNEnDP5y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3f71d909",
      "metadata": {
        "id": "3f71d909"
      },
      "source": [
        "### Value Model\n",
        "\n",
        "The value model is an crtic model in the actor-critic algorithm. It is defined as a simple neural network with two three layers. It is used here to get an estimate for the total expected value in the long term from the latent state. As seen in the paper:\n",
        "\n",
        "$v_\\psi(s_\\tau) \\approx\n",
        "\\mathbb{E}_{q}\\!\\left[\n",
        "  \\sum_{n = \\tau}^{t + H}\n",
        "  \\gamma^{n - \\tau} r_n\n",
        "\\right]$\n",
        "\n",
        "Similarly, the critic does not interact with the actual environment either, but only with the latent state during imagination."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "def __init__(self, latent_size, hidden_size, activation=nn.ELU):\n",
        "    super().__init__()\n",
        "    self.latent_size = latent_size\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(latent_size, hidden_size),\n",
        "        activation(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        activation(),\n",
        "        nn.Linear(hidden_size, a1)\n",
        "    )"
      ],
      "metadata": {
        "id": "B-3Ig-ISC8DR"
      },
      "id": "B-3Ig-ISC8DR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how the critic is trained during imagination. We optimize the model by trying to match it to returns from many future steps. The objective from the paper for critic's parameters $\\psi$ is:\n",
        "\n",
        "$\\min_{\\psi} \\;\n",
        "\\mathbb{E}_{q_\\theta, q_\\phi}\\!\\left[\n",
        "  \\sum_{\\tau = t}^{t + H}\n",
        "  \\frac{1}{2}\n",
        "  \\,\n",
        "  \\|v_\\psi(s_\\tau) - V_\\lambda(s_\\tau)\\|^2\n",
        "\\right]$\n",
        "\n",
        "where $V_\\lambda(s_\\tau)$ is an exponentially weighted average of estimated rewards beyond k steps with the learned value model for different k."
      ],
      "metadata": {
        "id": "cJjh3s_ffeTP"
      },
      "id": "cJjh3s_ffeTP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiation of the value model\n",
        "self.critic = models.Critic(self.config.main.stochastic_size + self.config.main.deterministic_size,\n",
        "                                  self.config.main.hidden_units).to(self.device)\n",
        "\n",
        "# Get action during imagination from the actor model\n",
        "action = self.actor(state, deterministics)\n",
        "\n",
        "# Compute loss by matching future returns\n",
        "values_dist = self.critic(state_trajectories[:, :-1].detach(), deterministics_trajectories[:, :-1].detach())\n",
        "\n",
        "critic_loss = -(discount.squeeze() * values_dist.log_prob(returns.detach())).mean()\n",
        "\n",
        "self.critic_optimizer.zero_grad()\n",
        "critic_loss.backward()\n",
        "nn.utils.clip_grad_norm_(\n",
        "    self.critic.parameters(),\n",
        "    self.config.main.clip_grad,\n",
        "    norm_type=self.config.main.grad_norm_type,\n",
        ")\n",
        "self.critic_optimizer.step()"
      ],
      "metadata": {
        "id": "Q0kiMAaWDJ7P"
      },
      "id": "Q0kiMAaWDJ7P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6bcfe123",
      "metadata": {
        "id": "6bcfe123"
      },
      "source": [
        "## Latent Imagination\n",
        "\n",
        "Instead of running rollouts on the actual environment, the Dreamer agent simulates subsequent trajectories within the latent space. This process is known as latent imagination.\n",
        "\n",
        "We start off with the state which represents the stochastic part and determinsitcs which represents the deterministic part of the latent state which initally come form the Representation Model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stochastic\n",
        "state = state.reshape(-1, self.config.main.stochastic_size)\n",
        "\n",
        "#Deterministic\n",
        "deterministics = deterministics.reshape(-1, self.config.main.deterministic_size)"
      ],
      "metadata": {
        "id": "ErVaN2yBSmZz"
      },
      "id": "ErVaN2yBSmZz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then use the state and deterministics to feed into the actor model in order to give us the action that our agent would take. Now with our state, deteminsiitcs, and action, we use the Representaiton Model, in this case the recurrent RSSM, in order to get an updated version of the resulting determinsitic hidden state. Conditioned on this new deterministic state, the transition RSSM predicts the new stochastic state (still within the latent space). Keep in mind that the agent is not actually stepping through the environment, rather just using what it has learned so far to speculate what could happen if it took certain actions."
      ],
      "metadata": {
        "id": "LdLIU2VETV9C"
      },
      "id": "LdLIU2VETV9C"
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(self.config.main.horizon):\n",
        "    action = self.actor(state, deterministics)\n",
        "    deterministics = self.rssm.recurrent(state, action, deterministics)\n",
        "    _, state = self.rssm.transition(deterministics)\n",
        "    state_trajectories[:, t, :] = state\n",
        "    deterministics_trajectories[:, t, :] = deterministics"
      ],
      "metadata": {
        "id": "kYHlkYldSq-I"
      },
      "id": "kYHlkYldSq-I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with the imagined latent states from the state and deterministics trajectories, we can use the reward model to give us an estimate of the expected reward along this path. Similarly, we use the critic to estimate the expected value from the state and deterministics trajectories."
      ],
      "metadata": {
        "id": "5m4km4V6WvRp"
      },
      "id": "5m4km4V6WvRp"
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = self.reward(state_trajectories, deterministics_trajectories)\n",
        "rewards_dist = torch.distributions.Normal(rewards, 1)\n",
        "rewards_dist = torch.distributions.Independent(rewards_dist, 1)\n",
        "rewards = rewards_dist.mode\n",
        "\n",
        "# continue is set whether or not this episode should keep going or stop\n",
        "if self.config.main.continue_loss:\n",
        "    _, conts_dist = self.cont_net(state_trajectories, deterministics_trajectories)\n",
        "    continues = conts_dist.mean\n",
        "else:\n",
        "    continues = self.config.main.discount * torch.ones_like(rewards)\n",
        "\n",
        "values = self.critic(state_trajectories, deterministics_trajectories).mode"
      ],
      "metadata": {
        "id": "2iMLEEjhSvOK"
      },
      "id": "2iMLEEjhSvOK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With these rewards and values, we can calculate the estimated returns."
      ],
      "metadata": {
        "id": "bNPcMbzeXbPL"
      },
      "id": "bNPcMbzeXbPL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns calculated using temporal difference bootstrapping\n",
        "# td_lambda uses gamma (discount factor) and lambda to compute multi-step returns\n",
        "returns = td_lambda(\n",
        "    rewards,\n",
        "    continues,\n",
        "    values,\n",
        "    self.config.main.lambda_,\n",
        "    self.device\n",
        ")"
      ],
      "metadata": {
        "id": "pduy_WNsWPEx"
      },
      "id": "pduy_WNsWPEx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can finally we can use the results to help our models learn. Here we optimize our actor to maximize the expected returns using gradient descent. We also improve the critic's value prediction using the predicted value distribution."
      ],
      "metadata": {
        "id": "8oKXE4BSam6v"
      },
      "id": "8oKXE4BSam6v"
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizing the Actor Model\n",
        "actor_loss = -(discount * returns).mean()\n",
        "\n",
        "self.actor_optimizer.zero_grad()\n",
        "actor_loss.backward()\n",
        "nn.utils.clip_grad_norm_(\n",
        "    self.actor.parameters(),\n",
        "    self.config.main.clip_grad,\n",
        "    norm_type=self.config.main.grad_norm_type,\n",
        ")\n",
        "self.actor_optimizer.step()\n",
        "\n",
        "\n",
        "#Optimizing the Critic Model\n",
        "values_dist = self.critic(state_trajectories[:, :-1].detach(), deterministics_trajectories[:, :-1].detach())\n",
        "\n",
        "critic_loss = -(discount.squeeze() * values_dist.log_prob(returns.detach())).mean()\n",
        "\n",
        "self.critic_optimizer.zero_grad()\n",
        "critic_loss.backward()\n",
        "nn.utils.clip_grad_norm_(\n",
        "    self.critic.parameters(),\n",
        "    self.config.main.clip_grad,\n",
        "    norm_type=self.config.main.grad_norm_type,\n",
        ")\n",
        "self.critic_optimizer.step()"
      ],
      "metadata": {
        "id": "2nesPL9sWYaQ"
      },
      "id": "2nesPL9sWYaQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Environment Interaction and the Full Training Loop\n",
        "\n",
        "After learning both the **world model** and the **behavior policy**, the final phase of Dreamer’s training loop is **Environment Interaction**,  \n",
        "where the agent uses its current policy to gather new real experiences. These experiences are then added to the replay buffer and used in the next round of Dynamics and Behavior Learning.\n",
        "\n",
        "This is the workflow:\n",
        "\n",
        "1. The agent **collects real experience** from the environment using the current policy (the Actor).  \n",
        "2. That experience is **stored in a replay buffer**.  \n",
        "3. The **world model** is updated using this real data (Dynamics Learning).  \n",
        "4. The **Actor** and **Critic** are then improved inside the world model using imagined trajectories (Behavior Learning).  \n",
        "5. The updated policy is used again to collect more experience — and the cycle repeats.\n",
        "\n",
        "This loop allows Dreamer to keep learning efficiently, balancing imagination with real-world data.  \n",
        "By alternating between these steps, the agent continually refines both its understanding of the environment and the quality of its decisions.\n"
      ],
      "metadata": {
        "id": "MNd2tp75CUiL"
      },
      "id": "MNd2tp75CUiL"
    },
    {
      "cell_type": "code",
      "source": [
        "def environment_interaction(self, env, replay_buffer):\n",
        "\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Encode observation and get latent state\n",
        "        latent_state = self.encoder(torch.tensor(observation).unsqueeze(0).float())\n",
        "        deter_state = self.rssm.init_deterministic(1)\n",
        "        stoch_state = self.rssm.init_stochastic(1)\n",
        "\n",
        "        # Actor chooses action based on current latent state\n",
        "        action = self.actor(stoch_state, deter_state)\n",
        "        action = action.detach().cpu().numpy()\n",
        "\n",
        "        # Step in the real environment\n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Store the transition in replay buffer\n",
        "        replay_buffer.add(observation, action, reward, next_obs)\n",
        "\n",
        "        # Move to next step\n",
        "        observation = next_obs\n"
      ],
      "metadata": {
        "id": "s8e0aZfxDYzv"
      },
      "id": "s8e0aZfxDYzv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This final phase closes Dreamer’s training loop.\n",
        "  \n",
        "After training its world model and policy, Dreamer uses the current Actor to interact with the real environment, collecting new experience tuples `(observation, action, reward, next_observation)` that are stored in a replay buffer. These samples are then used in the next round of Dynamics and Behavior Learning.  \n",
        "\n",
        "By repeating this cycle — **collect → model → imagine → improve** —  Dreamer continually refines both its understanding of the world and its ability to act within it.\n"
      ],
      "metadata": {
        "id": "uuxVT_vtDs2s"
      },
      "id": "uuxVT_vtDs2s"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}